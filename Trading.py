# -*- coding: utf-8 -*-
"""SG_MMAE500_FP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rRgBzmAEC3KuTnZaLHAR7ig9Dn7Cnv2K
"""

from IPython.display import set_matplotlib_formats
set_matplotlib_formats('pdf', 'svg')

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import yfinance as yf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import gym
import gym_anytrading
import pyfolio as pf
import tensorflow as tf
import quantstats as qs
import pandas as pd

from stable_baselines import DQN
from stable_baselines.ddpg.policies import DDPGPolicy
from stable_baselines.common.policies import MlpPolicy
from stable_baselines.common.vec_env import DummyVecEnv

# Get Apple stock data
df = yf.download("AAPL",start='2020-01-01',end='2021-01-01')
df = df.dropna()

print(df)

# Get Adjusted Close price (normalized stock returns)
ac = df['Adj Close']

# Set & build the gym environment, using gym-anytrading stocks environment
window_size = 5
env = gym.make('stocks-v0', df=df, frame_bound=(10,100), window_size=5)

# Observe the maximum possible profit we can make in this period:
print("> max_possible_profit:", env.max_possible_profit())

state = env.reset()
while True: 
    action = env.action_space.sample()
    n_state, reward, done, info = env.step(action)
    if done: 
        print("info", info)
        break

# Let's test the environment       
plt.figure(figsize=(15,6))

# Red dots on graph are shorts, green are longs.
plt.cla()
env.render_all()
plt.title('Time vs AAPL Price')
plt.xlabel('Time (days)')
plt.ylabel('AAPL Price ($)')
plt.show()

env_maker = lambda: gym.make('stocks-v0', df=df, frame_bound=(5,100), window_size=5)
env = DummyVecEnv([env_maker])

def train_DQN(env_train, model_name, timesteps=100000):
 # DQN 
 kwargs = {'double_q': False, 'prioritized_replay': False, 'policy_kwargs': dict(dueling=False)}
 model = DQN('MlpPolicy', env_train, verbose=0, **kwargs)
 model.learn(total_timesteps=timesteps)
 return model

def train_DuelDQN(env_train, model_name, timesteps=100000):
 # Dueling DQN
 kwargs = {'double_q': False, 'prioritized_replay': False, 'policy_kwargs': dict(dueling=True)}
 model = DQN('MlpPolicy', env_train, verbose = 0, **kwargs)
 model.learn(total_timesteps=timesteps)
 return model

def train_DDQN(env_train, model_name, timesteps=100000):
 # Double DQN
 kwargs = {'double_q': True, 'prioritized_replay': False, 'policy_kwargs': dict(dueling=False)}
 model = DQN('MlpPolicy', env_train, verbose = 0, **kwargs)
 model.learn(total_timesteps=timesteps)
 return model

#model1 = train_DQN(env,'DQN')
#model2 = train_DuelDQN(env, 'DuelDQN')
model3 = train_DDQN(env, 'DDQN')

# Commenting and uncommenting to avoid clutter when running code.

env = gym.make('stocks-v0', df=df, frame_bound=(90,180), window_size=5)
obs = env.reset()
while True: 
    obs = obs[np.newaxis, ...]
    action, _states = model3.predict(obs)
    obs, rewards, done, info = env.step(action)
    if done:
        print("info", info)
        break

# Plot results after training
plt.figure(figsize=(15,6))
plt.cla()
env.render_all()
plt.title('Double DQN: Time vs AAPL Price')
plt.xlabel('Time (days)')
plt.ylabel('AAPL Price ($)')
plt.show()

start_index = 90
end_index = len(df)
qs.extend_pandas()

net_worth = pd.Series(env.history['total_profit'])
returns = net_worth.pct_change().iloc[1:]

#qs.reports.html(returns, output='dqn_quantstats.html')

x = np.arange(1,89)

plt.plot(x,returns)
plt.xlabel('Time (days)')
plt.ylabel('Returns (%)')
plt.title('Double DQN: Returns over Time')

Sharpe = qs.stats.sharpe(returns)
Profit_Ratio = qs.stats.profit_ratio(returns, prepare_returns = True)
Volatility = qs.stats.volatility(returns, periods = 90)
Outliers = qs.stats.outliers(returns, quantile = 0.95)
RoR = qs.stats.ror(returns)

# Store calculations in a dictionary
dic = {"Sharpe ratio": Sharpe, "Profit Ratio": Profit_Ratio, "Volatility": Volatility, "Risk of Ruin": RoR}
print(dic)
#print(Outliers)

# Convert code to PDF
!wget -nc https://raw.githubusercontent.com/brpy/colab-pdf/master/colab_pdf.py
from colab_pdf import colab_pdf
colab_pdf('SG_MMAE500_FP.ipynb')

